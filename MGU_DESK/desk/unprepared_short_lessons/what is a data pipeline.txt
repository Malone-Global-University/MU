What is a Data Pipeline?
A data pipeline is a series of processing steps to prepare enterprise data for analysis. Organizations have a large volume of data from various sources like applications, Internet of Things (IoT) devices, and other digital channels. However, raw data is useless; it must be moved, sorted, filtered, reformatted, and analyzed for business intelligence. A data pipeline includes various technologies to verify, summarize, and find patterns in data to inform business decisions. Well-organized data pipelines support various big data projects, such as data visualizations, exploratory data analyses, and machine learning tasks.

What are the benefits of a data pipeline?
Data pipelines let you integrate data from different sources and transform it for analysis. They remove data silos and make your data analytics more reliable and accurate. Here are some key benefits of a data pipeline.

Improved data quality
Data pipelines clean and refine raw data, improving its usefulness for end users. They standardize formats for fields like dates and phone numbers while checking for input errors. They also remove redundancy and ensure consistent data quality across the organization.

Efficient data processing
Data engineers have to perform many repetitive tasks while transforming and loading data. Data pipelines allow them to automate data transformation tasks and focus instead on finding the best business insights. Data pipelines also help data engineers more quickly process raw data that loses value over time.

Comprehensive data integration
A data pipeline abstracts data transformation functions to integrate data sets from disparate sources. It can cross-check values of the same data from multiple sources and fix inconsistencies. For example, imagine that the same customer makes a purchase from your ecommerce platform and your digital service. However, they misspell their name in the digital service. The pipeline can fix this inconsistency before sending the data for analytics.

How does a data pipeline work?
Just like a water pipeline moves water from the reservoir to your taps, a data pipeline moves data from the collection point to storage. A data pipeline extracts data from a source, makes changes, then saves it in a specific destination. We explain the critical components of data pipeline architecture below.

Data sources
A data source can be an application, a device, or another database. Disparate sources may push data into the pipeline. The pipeline may also extract data points using an API call, webhook, or data duplication process. You can synchronize data extraction for real-time processing or collect data in scheduled intervals from your data sources.

Transformations
As raw data flows through the pipeline, it changes to become more useful for business intelligence. Transformations are operations—such as sorting, reformatting, deduplication, verification, and validation—that change data. Your pipeline can filter, summarize, or process data to meet your analysis requirements.

Dependencies
As changes happen sequentially, specific dependencies may exist that reduce the speed of moving data in the pipeline. There are two main types of dependencies—technical and business. For example, if the pipeline has to wait for a central queue to fill up before proceeding, it’s a technical dependency. Conversely, if the pipeline has to pause until another business unit cross-verifies the data, it’s a business dependency.

Destinations
The endpoint of your data pipeline can be a data warehouse, data lake, or another business intelligence or data analysis application. Sometimes the destination is also called a data sink.



What are the types of data pipelines?
There are two main types of data pipelines—stream processing pipelines and batch processing pipelines.

Stream processing pipelines
A data stream is a continuous, incremental sequence of small-sized data packets. It usually represents a series of events occurring over a given period. For example, a data stream could show sensor data containing measurements over the last hour. A single action, like a financial transaction, can also be called an event. Streaming pipelines process a series of events for real-time analytics.

Streaming data requires low latency and high fault tolerance. Your data pipeline should be able to process data even if some data packets are lost or arrive in a different order than expected.

Batch processing pipelines
Batch processing data pipelines process and store data in large volumes or batches. They are suitable for occasional high-volume tasks like monthly accounting.

The data pipeline contains a series of sequenced commands, and every command is run on the entire batch of data. The data pipeline gives the output of one command as the input to the following command. After all data transformations are complete, the pipeline loads the entire batch into a cloud data warehouse or another similar data store.

Read about batch processing »

Difference between batch and streaming data pipelines
Batch processing pipelines run infrequently and typically during off-peak hours. They require high computing power for a short period when they run. In contrast, stream processing pipelines run continuously but require low computing power. Instead, they need reliable, low-latency network connections.

What is the difference between data pipelines and ETL pipelines?
An extract, transform, and load (ETL) pipeline is a special type of data pipeline. ETL tools extract or copy raw data from multiple sources and store it in a temporary location called a staging area. They transform data in the staging area and load it into data lakes or warehouses.

Not all data pipelines follow the ETL sequence. Some may extract the data from a source and load it elsewhere without transformations. Other data pipelines follow an extract, load, and transform (ELT) sequence, where they extract and load unstructured data directly into a data lake. They perform changes after moving the information to cloud data warehouses.

Read about ETL »



What is batch processing?
Batch processing is the method computers use to periodically complete high-volume, repetitive data jobs. Certain data processing tasks, such as backups, filtering, and sorting, can be compute intensive and inefficient to run on individual data transactions. Instead, data systems process such tasks in batches, often in off-peak times when computing resources are more commonly available, such as at the end of the day or overnight. For example, consider an ecommerce system that receives orders throughout the day. Instead of processing every order as it occurs, the system might collect all orders at the end of each day and share them in one batch with the order fulfillment team.

Why is batch processing important?
Organizations use batch processing because it requires minimal human interaction and makes repetitive tasks more efficient to run. You can set up batches of jobs composed of millions of records to be worked through together when compute power is most readily available, putting less stress on your systems. Modern batch processing also requires minimal human supervision or management. If there is an issue, the system automatically notifies the concerned team to solve it. Managers take a hands-off approach, trusting their batch processing software to do its job. More benefits of batch processing follow.

What is the history of batch processing?
Batch processing is more than a century old, although the technicalities of how it works have continually evolved. The first instance of batch processing dates back to 1890, when an electronic tabulator was used to record information for the United States Census Bureau. Census workers marked data cards—called punch cards—and processed them in batches through an electromechanical device. By the 1960s, developers could schedule batch programs on magnetic tape for computers to run sequentially throughout the day. Batch jobs also became commonplace as the mainframe computer improved and became more powerful and efficient. Modern organizations use software-based batch applications for common business processes such as generating reports, printing documents, or updating information at the end of the day.

What are examples of jobs batch processing can automate?
Batch process systems are used to process various types of data and requests. Some of the most common types of batch processing jobs include:

Weekly/monthly billing
Payroll
Inventory processing
Report generation
Data conversion
Subscription cycles
Supply chain fulfillment
What are some use cases of batch processing systems?
There are multiple use cases of batch processing systems. Key examples follow.

Financial services
Financial services organizations, from agile financial technologies to legacy enterprises, have been using batch processing in areas such as high performance computing for risk management, end-of-day transaction processing, and fraud surveillance. They use batch processing to minimize human error, increase speed and accuracy, and reduce costs with automation.

Software as a service
Enterprises that deliver software as a service (SaaS) applications often run into issues when it comes to scalability. Using batch processing, you can scale customer demand while automating job scheduling. Creating containerized application environments to scale demand for high-volume processing is a project that can take months or even years to complete, but batch processing systems help you achieve the same result in a much shorter timeframe.

Medical research
Analysis of large amounts of data—or big data—is a common requirement in the field of research. You can apply batch processing in data analytics applications such as computational chemistry, clinical modeling, molecular dynamics, and genomic sequencing testing and analysis. For example, scientists use batch processing to capture better data to begin drug design and gain a deeper understanding of the role of a particular biochemical process. 

Digital media
Media and entertainment enterprises require highly scalable batch processing systems to automatically process data—such as files, graphics, and visual effects—for high-resolution video content. You can use batch processing to accelerate content creation, dynamically scale media packaging, and automate media workload.

How does batch processing work?
While batch processing applications vary depending on the type of task that needs to be done, the basics of any batch job remain the same. The user can run batch jobs by specifying the following details:

Name of the person submitting the job
Batch processes or programs that need to run
System location of the data input
System location for processed data output
Time, or batch window, when the batch job should be run
The user also specifies the batch size, or the number of work units that the system needs to process in one complete batch operation. Some examples of batch size include: 

Number of batch file lines to read and store in the database
Number of messages to read and process from a queue
Number of transactions to sort and send to the next application
During the batch window, the batch processing system uses the batch size information to allocate the resources needed to run the batch job efficiently. Modern systems can run hundreds of thousands of batch jobs on premises or in the cloud.

Dependencies
Batch job tasks can run sequentially or simultaneously. Sequences can differ depending on whether an earlier task is completed successfully. Examples of dependencies include a customer making an order in an online store or paying a bill. A dependency can also be set up to initiate a job processing cycle.

Cron commands
A cron command is a batch job that runs regularly. You can set up recurrence patterns for batch jobs—for example, setting up a job to invoice for subscriptions at the end of every month.

How can you monitor batch processing?
While batch processing systems work with minimal input from personnel, they still need some oversight. To monitor batch processes, you can set up alerts—or exceptions—that are sent when the batch job succeeds, fails, or has finished running.

Monitors
Monitors in batch processes look for abnormalities, such as a job taking longer than it should to complete. In this instance, it would stop the next job from beginning and inform the relevant staff of the exception.

Post-processing analysis
You can view the history of a batch job after it has been processed. Most batch processes include log files that record messages while the job was running.



What is the difference between batch processing and stream processing?
Whereas batch systems process large volumes of data and requests in sequential order, stream processing continually analyzes data that flows through a system or between devices. Stream processing monitors real-time data and continually passes it on in the network. It requires more processing power to monitor the large amounts of data.

When the size of data being streamed is not known or infinite, streaming data can be preferable to batch processing. As a result, stream processing is commonly used for business functions such as cybersecurity, Internet of Things (IoT), personalized marketing services, and log monitoring.

Given their complementary capabilities, some enterprises have implemented a hybrid system that includes batch processing and stream processing in their daily operations.

What is ETL?
Extract, transform, and load (ETL) is the process of combining data from multiple sources into a large, central repository called a data warehouse. ETL uses a set of business rules to clean and organize raw data and prepare it for storage, data analytics, and machine learning (ML). You can address specific business intelligence needs through data analytics (such as predicting the outcome of business decisions, generating reports and dashboards, reducing operational inefficiency, and more).

Why is ETL important?
Organizations today have both structured and unstructured data from various sources including:

Customer data from online payment and customer relationship management (CRM) systems
Inventory and operations data from vendor systems
Sensor data from Internet of Things (IoT) devices
Marketing data from social media and customer feedback
Employee data from internal human resources systems
By applying the process of extract, transform, and load (ETL), individual raw datasets can be prepared in a format and structure that is more consumable for analytics purposes, resulting in more meaningful insights. For example, online retailers can analyze data from points of sale to forecast demand and manage inventory. Marketing teams can integrate CRM data with customer feedback on social media to study consumer behavior.

How does ETL benefit business intelligence?
Extract, transform, and load (ETL) improves business intelligence and analytics by making the process more reliable, accurate, detailed, and efficient.

Historical context
ETL gives deep historical context to the organization’s data. An enterprise can combine legacy data with data from new platforms and applications. You can view older datasets alongside more recent information, which gives you a long-term view of data.

Consolidated data view
ETL provides a consolidated view of data for in-depth analysis and reporting. Managing multiple datasets demands time and coordination and can result in inefficiencies and delays. ETL combines databases and various forms of data into a single, unified view. The data integration process improves the data quality and saves the time required to move, categorize, or standardize data. This makes it easier to analyze, visualize, and make sense of large datasets.

Accurate data analysis
ETL gives more accurate data analysis to meet compliance and regulatory standards. You can integrate ETL tools with data quality tools to profile, audit, and clean data, ensuring that the data is trustworthy.

Task automation
ETL automates repeatable data processing tasks for efficient analysis. ETL tools automate the data migration process, and you can set them up to integrate data changes periodically or even at runtime. As a result, data engineers can spend more time innovating and less time managing tedious tasks like moving and formatting data.

How has ETL evolved?
Extract, transform, and load (ETL) originated with the emergence of relational databases that stored data in the form of tables for analysis. Early ETL tools attempted to convert data from transactional data formats to relational data formats for analysis.

Traditional ETL
Raw data was typically stored in transactional databases that supported many read and write requests but did not lend well to analytics. You can think of it as a row in a spreadsheet. For example, in an ecommerce system, the transactional database stored the purchased item, customer details, and order details in one transaction. Over the year, it contained a long list of transactions with repeat entries for the same customer who purchased multiple items during the year. Given the data duplication, it became cumbersome to analyze the most popular items or purchase trends in that year.

To overcome this issue, ETL tools automatically converted this transactional data into relational data with interconnected tables. Analysts could use queries to identify relationships between the tables, in addition to patterns and trends.

Modern ETL
As ETL technology evolved, both data types and data sources increased exponentially. Cloud technology emerged to create vast databases (also called data sinks). Such data sinks can receive data from multiple sources and have underlying hardware resources that can scale over time. ETL tools have also become more sophisticated and can work with modern data sinks. They can convert data from legacy data formats to modern data formats. Examples of modern databases follow.

Data warehouses
A data warehouse is a central repository that can store multiple databases. Within each database, you can organize your data into tables and columns that describe the data types in the table. The data warehouse software works across multiple types of storage hardware—such as solid state drives (SSDs), hard drives, and other cloud storage—to optimize your data processing.

Data lakes
With a data lake, you can store your structured and unstructured data in one centralized repository and at any scale. You can store data as is without having to first structure it based on questions you might have in the future. Data lakes also allow you to run different types of analytics on your data, like SQL queries, big data analytics, full-text search, real-time analytics, and machine learning (ML) to guide better decisions.

How does ETL work?
Extract, transform, and load (ETL) works by moving data from the source system to the destination system at periodic intervals. The ETL process works in three steps:

Extract the relevant data from the source database
Transform the data so that it is better suited for analytics
Load the data into the target database


What is data extraction?
In data extraction, extract, transform, and load (ETL) tools extract or copy raw data from multiple sources and store it in a staging area. A staging area (or landing zone) is an intermediate storage area for temporarily storing extracted data. Data staging areas are often transient, meaning their contents are erased after data extraction is complete. However, the staging area might also retain a data archive for troubleshooting purposes.

How frequently the system sends data from the data source to the target data store depends on the underlying change data capture mechanism. Data extraction commonly happens in one of the three following ways.

Update notification
In update notification, the source system notifies you when a data record changes. You can then run the extraction process for that change. Most databases and web applications provide update mechanisms to support this data integration method.

Incremental extraction
Some data sources can't provide update notifications but can identify and extract data that has been modified over a given time period. In this case, the system checks for changes at periodic intervals, such as once a week, once a month, or at the end of a campaign. You only need to extract data that has changed.

Full extraction
Some systems can't identify data changes or give notifications, so reloading all data is the only option. This extraction method requires you to keep a copy of the last extract to check which records are new. Because this approach involves high data transfer volumes, we recommend you use it only for small tables.

What is data transformation?
In data transformation, extract, transform, and load (ETL) tools transform and consolidate the raw data in the staging area to prepare it for the target data warehouse. The data transformation phase can involve the following types of data changes.

Basic data transformation
Basic transformations improve data quality by removing errors, emptying data fields, or simplifying data. Examples of these transformations follow.

Data cleansing
Data cleansing removes errors and maps source data to the target data format. For example, you can map empty data fields to the number 0, map the data value “Parent” to “P,” or map “Child” to “C.”

Data deduplication
Deduplication in data cleansing identifies and removes duplicate records.

Data format revision
Format revision converts data, such as character sets, measurement units, and date/time values, into a consistent format. For example, a food company might have different recipe databases with ingredients measured in kilograms and pounds. ETL will convert everything to pounds.

Advanced data transformation
Advanced transformations use business rules to optimize the data for easier analysis. Examples of these transformations follow.

Derivation
Derivation applies business rules to your data to calculate new values from existing values. For example, you can convert revenue to profit by subtracting expenses or calculating the total cost of a purchase by multiplying the price of each item by the number of items ordered.

Joining
In data preparation, joining links the same data from different data sources. For example, you can find the total purchase cost of one item by adding the purchase value from different vendors and storing only the final total in the target system.

Splitting
You can divide a column or data attribute into multiple columns in the target system. For example, if the data source saves the customer name as “Jane John Doe,” you can split it into a first, middle, and last name.

Summarization
Summarization improves data quality by reducing a large number of data values into a smaller dataset. For example, customer order invoice values can have many different small amounts. You can summarize the data by adding them up over a given period to build a customer lifetime value (CLV) metric.

Encryption
You can protect sensitive data to comply with data laws or data privacy by adding encryption before the data streams to the target database.

What is data loading?
In data loading, extract transform, and load (ETL) tools move the transformed data from the staging area into the target data warehouse. For most organizations that use ETL, the process is automated, well defined, continual, and batch driven. Two methods for loading data follow.

Full load
In full load, the entire data from the source is transformed and moved to the data warehouse. The full load usually takes place the first time you load data from a source system into the data warehouse.

Incremental load 
In incremental load, the ETL tool loads the delta (or difference) between target and source systems at regular intervals. It stores the last extract date so that only records added after this date are loaded. There are two ways to implement incremental load.

Streaming incremental load
If you have small data volumes, you can stream continual changes over data pipelines to the target data warehouse. When the speed of data increases to millions of events per second, you can use event stream processing to monitor and process the data streams to make more-timely decisions.

Batch incremental load
If you have large data volumes, you can collect load data changes into batches periodically. During this set period of time, no actions can happen to either the source or target system as data is synchronized.

What is ELT?
Extract, load, and transform (ELT) is an extension of extract, transform, and load (ETL) that reverses the order of operations. You can load data directly into the target system before processing it. The intermediate staging area is not required because the target data warehouse has data mapping capabilities within it. ELT has become more popular with the adoption of cloud infrastructure, which gives target databases the processing power they need for transformations.

ETL compared to ELT
ELT works well for high-volume, unstructured datasets that require frequent loading. It is also ideal for big data because the planning for analytics can be done after data extraction and storage. It leaves the bulk of transformations for the analytics stage and focuses on loading minimally processed raw data into the data warehouse.

The ETL process requires more definition at the beginning. Analytics needs to be involved from the start to define target data types, structures, and relationships. Data scientists mainly use ETL to load legacy databases into the warehouse, and ELT has become the norm today.

What is data virtualization?
Data virtualization uses a software abstraction layer to create an integrated data view without physically extracting, transforming, or loading the data. Organizations use this functionality as a virtual unified data repository without the expense and complexity of building and managing separate platforms for source and target. While you can use data virtualization alongside extract, transform, and load (ETL), it is increasingly seen as an alternative to ETL and other physical data integration methods. For example, you can use AWS Glue Elastic Views to quickly create a virtual table—a materialized view—from multiple different source data stores.

What is AWS Glue?
AWS Glue is a serverless data integration service that makes it easier for analytics users to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning, and application development.

You can discover and connect to 80+ diverse data stores.
You can manage your data in a centralized data catalog.
Data Engineers, ETL developers, data analysts, and business users can use AWS Glue Studio to create, run, and monitor ETL pipelines to load data into data lakes.
AWS Glue Studio offers Visual ETL, Notebook, and code editor interfaces, so users have tools appropriate to their skillsets.
With Interactive Sessions, data engineers can explore data as well as author and test jobs using their preferred IDE or notebook.
AWS Glue is serverless and automatically scales on demand, so you can focus on gaining insights from petabyte-scale data without managing infrastructure.
Get started with AWS Glue by creating an AWS account.